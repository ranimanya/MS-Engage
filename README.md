# MyVision-MS-Engage-Project 
* **Problem**: Develop a browser-based application or a native mobile application to demonstrate application of Face Recognition technology.
* **My idea**: In face-to-face social interactions, *blind and visually impaired persons* lack access to nonverbal cues like *facial expressions*, gestures and other general details such as *age*, *gender* and *race* which may lead to impaired interpersonal communication. 
* **Solution**: Here is the design and implementation of a face detection and recognition system for the visually impaired. 
Here, I attempted to build an application which gives *voice output to the user to facilitate interaction*
* **Technology used**: Python, Opencv, Deepface, gTTs(Google-Text-To-Speech), Kivy, Pycharm IDE
## How to run?
Go to requirements.txt for related dependencies.
Clone the project and open on your IDE, click and run **MyVision.py**

<img width="960" alt="image" src="https://user-images.githubusercontent.com/81632252/170852115-3ee89dc3-5633-4a3b-8312-65475084bd5b.png">
<img width="960" alt="image" src="https://user-images.githubusercontent.com/81632252/170852190-c4237307-e185-4741-be91-5dfe317a500c.png">
<img width="616" alt="msengage22" src="https://user-images.githubusercontent.com/81632252/170880184-bfcf9d65-d77b-4550-b041-e0445c28b9aa.png">
<img width="531" alt="msengage223" src="https://user-images.githubusercontent.com/81632252/170880188-79a2556d-286c-478e-82b8-210dedaccf29.png">


## References used
* https://pypi.org/project/deepface/
* https://kivy.org/doc/stable/examples/index.html

